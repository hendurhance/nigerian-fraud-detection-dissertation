{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIBSS Fraud Detection: Data Preprocessing and Pipeline Setup\n",
    "\n",
    "This notebook covers the initial data preprocessing and pipeline setup for the NIBSS fraud detection system.\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not available\n",
    "!pip install -q \\\n",
    "    scikit-learn>=1.4.0 \\\n",
    "    imbalanced-learn>=0.12.0 \\\n",
    "    category-encoders>=2.7.0 \\\n",
    "    shap==0.45.0 \\\n",
    "    xgboost==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score,\n",
    "    confusion_matrix, classification_report, precision_recall_curve,\n",
    "    roc_curve, auc, make_scorer\n",
    ")\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"All packages imported successfully!\")\n",
    "print(f\"Current time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Generated Dataset\n",
    "\n",
    "Load the synthetic NIBSS fraud dataset generated using our dataset generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../data/processed/nibss_fraud_dataset.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['is_fraud'].mean():.4f}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Basic validation\n",
    "print(\"\\nColumn types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "numerical_features = [\n",
    "    'amount', 'tx_count_24h', 'amount_sum_24h', 'amount_mean_7d',\n",
    "    'amount_std_7d', 'tx_count_total', 'amount_mean_total', 'amount_std_total',\n",
    "    'channel_diversity', 'location_diversity', 'amount_vs_mean_ratio',\n",
    "    'online_channel_ratio', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "    'month_sin', 'month_cos', 'amount_log', 'amount_rounded',\n",
    "    'velocity_score', 'merchant_risk_score', 'composite_risk'\n",
    "]\n",
    "\n",
    "categorical_low_cardinality = [\n",
    "    'channel', 'age_group', 'is_weekend', 'is_peak_hour'\n",
    "]\n",
    "\n",
    "categorical_high_cardinality = [\n",
    "    'merchant_category', 'bank', 'location'\n",
    "]\n",
    "\n",
    "# Additional time-based features (already in dataset but let's verify)\n",
    "time_features = ['hour', 'day_of_week', 'month']\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = numerical_features + categorical_low_cardinality + categorical_high_cardinality + time_features\n",
    "X = df[feature_columns].copy()\n",
    "y = df['is_fraud'].copy()\n",
    "\n",
    "print(f\"Feature columns: {len(feature_columns)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_low_cardinality + categorical_high_cardinality + time_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split to maintain fraud rate\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=RANDOM_SEED, stratify=y_temp\n",
    ")  # 0.176 * 0.85 â‰ˆ 0.15\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Train: {X_train.shape[0]} samples, fraud rate: {y_train.mean():.4f}\")\n",
    "print(f\"Val: {X_val.shape[0]} samples, fraud rate: {y_val.mean():.4f}\")\n",
    "print(f\"Test: {X_test.shape[0]} samples, fraud rate: {y_test.mean():.4f}\")\n",
    "\n",
    "# Save indices for reproducibility\n",
    "split_indices = {\n",
    "    'train': X_train.index.tolist(),\n",
    "    'val': X_val.index.tolist(),\n",
    "    'test': X_test.index.tolist()\n",
    "}\n",
    "joblib.dump(split_indices, '../data/processed/split_indices.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Identify categorical columns for SMOTENC\n",
    "categorical_indices = []\n",
    "all_features = numerical_features + categorical_low_cardinality + categorical_high_cardinality + time_features\n",
    "\n",
    "for i, col in enumerate(all_features):\n",
    "    if col in categorical_low_cardinality + categorical_high_cardinality + time_features:\n",
    "        categorical_indices.append(i)\n",
    "\n",
    "print(f\"Categorical indices for SMOTENC: {categorical_indices}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat_low', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_low_cardinality),\n",
    "        ('cat_high', TargetEncoder(smoothing=10), categorical_high_cardinality),\n",
    "        ('time', 'passthrough', time_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Pipelines with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SMOTE strategy for 1:5 ratio\n",
    "n_fraud_train = y_train.sum()\n",
    "n_legit_train = len(y_train) - n_fraud_train\n",
    "desired_fraud_samples = n_legit_train // 5\n",
    "sampling_strategy = desired_fraud_samples / n_legit_train\n",
    "\n",
    "print(f\"Original fraud samples in train: {n_fraud_train}\")\n",
    "print(f\"Original legitimate samples in train: {n_legit_train}\")\n",
    "print(f\"SMOTE sampling strategy: {sampling_strategy:.4f}\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'logistic_regression': LogisticRegression(\n",
    "        max_iter=500,  # Reduced from 1000\n",
    "        random_state=RANDOM_SEED,\n",
    "        solver='liblinear',  # Faster solver\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'random_forest': RandomForestClassifier(\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'xgboost': XGBClassifier(\n",
    "        random_state=RANDOM_SEED,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        tree_method='hist',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create pipelines with SMOTE\n",
    "pipelines = {}\n",
    "for name, model in models.items():\n",
    "    # Note: We use regular SMOTE here, but in production you might want SMOTENC\n",
    "    # for mixed data types. However, after preprocessing, all features are numeric\n",
    "    pipelines[name] = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(sampling_strategy=sampling_strategy, random_state=RANDOM_SEED)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "print(\"Model pipelines created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'logistic_regression': {\n",
    "        'classifier__C': [0.1, 1.0, 10.0],  # Reduced from 4 to 3\n",
    "        'classifier__penalty': ['l2']  # Only L2, L1 is slower with saga solver\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'classifier__n_estimators': [100, 200],  # Reduced from 3 to 2\n",
    "        'classifier__max_depth': [20, None],     # Reduced from 4 to 2\n",
    "        'classifier__min_samples_split': [10],   # Fixed at reasonable value\n",
    "        'classifier__max_features': ['sqrt']     # Fixed at most common choice\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'classifier__learning_rate': [0.1, 0.3],      # Reduced from 3 to 2\n",
    "        'classifier__max_depth': [6],                   # Fixed at good default\n",
    "        'classifier__n_estimators': [100, 200],        # Reduced from 3 to 2\n",
    "        'classifier__subsample': [1.0],                # Fixed\n",
    "        'classifier__colsample_bytree': [1.0]          # Fixed\n",
    "    }\n",
    "}\n",
    "\n",
    "# Custom scoring function for AUC-PR (more suitable for imbalanced data)\n",
    "average_precision_scorer = make_scorer(average_precision_score)\n",
    "\n",
    "print(\"Hyperparameter grids defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data and Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data info\n",
    "data_info = {\n",
    "    'n_samples': len(df),\n",
    "    'n_features': len(feature_columns),\n",
    "    'fraud_rate': y.mean(),\n",
    "    'numerical_features': numerical_features,\n",
    "    'categorical_low': categorical_low_cardinality,\n",
    "    'categorical_high': categorical_high_cardinality,\n",
    "    'time_features': time_features,\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val),\n",
    "    'test_size': len(X_test),\n",
    "    'smote_ratio': sampling_strategy\n",
    "}\n",
    "\n",
    "joblib.dump(data_info, '../data/processed/data_info.pkl')\n",
    "joblib.dump(pipelines, '../models/base_pipelines.pkl')\n",
    "joblib.dump(param_grids, '../config/param_grids.pkl')\n",
    "\n",
    "# Save data splits\n",
    "data_splits = {\n",
    "    'X_train': X_train,\n",
    "    'X_val': X_val,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test\n",
    "}\n",
    "joblib.dump(data_splits, '../data/processed/data_splits.pkl')\n",
    "\n",
    "print(\"All data and pipeline components saved successfully!\")\n",
    "print(\"Ready for hyperparameter optimization in next notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "gc.collect()\n",
    "print(\"Memory cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}