{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIBSS Fraud Detection: Model Optimization and Hyperparameter Tuning\n",
    "\n",
    "This notebook focuses on optimizing machine learning models for fraud detection using Grid Search and comprehensive evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score,\n",
    "    make_scorer, precision_score, recall_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Load saved data from previous notebook\n",
    "data_splits = joblib.load('../data/processed/data_splits.pkl')\n",
    "pipelines = joblib.load('../models/base_pipelines.pkl')\n",
    "param_grids = joblib.load('../config/param_grids.pkl')\n",
    "data_info = joblib.load('../data/processed/data_info.pkl')\n",
    "\n",
    "X_train = data_splits['X_train']\n",
    "y_train = data_splits['y_train']\n",
    "X_val = data_splits['X_val']\n",
    "y_val = data_splits['y_val']\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple scorers for comprehensive evaluation\n",
    "scoring = {\n",
    "    'auc_roc': make_scorer(roc_auc_score, needs_proba=True),\n",
    "    'auc_pr': make_scorer(average_precision_score, needs_proba=True),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score)\n",
    "}\n",
    "\n",
    "# Primary metric for optimization\n",
    "primary_metric = 'auc_roc'\n",
    "\n",
    "print(\"Scoring functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "cv_results = {}\n",
    "best_models = {}\n",
    "training_times = {}\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Optimize each model\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Optimizing {model_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grids[model_name],\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        refit=primary_metric,\n",
    "        n_jobs=-1,\n",
    "        verbose=3,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Store results\n",
    "    cv_results[model_name] = pd.DataFrame(grid_search.cv_results_)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    training_times[model_name] = time.time() - start_time\n",
    "\n",
    "    # Print best parameters and scores\n",
    "    print(f\"\\nBest parameters for {model_name}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"\\nBest CV scores:\")\n",
    "    print(f\"AUC-ROC: {grid_search.cv_results_[f'mean_test_{primary_metric}'][grid_search.best_index_]:.4f} \"\n",
    "          f\"(+/- {grid_search.cv_results_[f'std_test_{primary_metric}'][grid_search.best_index_]:.4f})\")\n",
    "    print(f\"AUC-PR: {grid_search.cv_results_['mean_test_auc_pr'][grid_search.best_index_]:.4f} \"\n",
    "          f\"(+/- {grid_search.cv_results_['std_test_auc_pr'][grid_search.best_index_]:.4f})\")\n",
    "    print(f\"F1-Score: {grid_search.cv_results_['mean_test_f1'][grid_search.best_index_]:.4f} \"\n",
    "          f\"(+/- {grid_search.cv_results_['std_test_f1'][grid_search.best_index_]:.4f})\")\n",
    "    print(f\"\\nTraining time: {training_times[model_name]:.2f} seconds\")\n",
    "\n",
    "    # Save intermediate results\n",
    "    joblib.dump(grid_search, f'../models/grid_search_{model_name}.pkl')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All models optimized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table 4.4 - Optimal Hyperparameters and Cross-Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the table\n",
    "table_data = []\n",
    "\n",
    "for model_name in ['logistic_regression', 'random_forest', 'xgboost']:\n",
    "    # Get best parameters\n",
    "    grid_search = joblib.load(f'../models/grid_search_{model_name}.pkl')\n",
    "    best_idx = grid_search.best_index_\n",
    "\n",
    "    # Format parameters\n",
    "    params = grid_search.best_params_\n",
    "    if model_name == 'logistic_regression':\n",
    "        param_str = f\"C = {params['classifier__C']}, penalty = '{params['classifier__penalty']}', class_weight = 'balanced'\"\n",
    "    elif model_name == 'random_forest':\n",
    "        param_str = (f\"n_estimators = {params['classifier__n_estimators']}, \"\n",
    "                    f\"max_depth = {params['classifier__max_depth']}, \"\n",
    "                    f\"min_samples_split = {params['classifier__min_samples_split']}, \"\n",
    "                    f\"max_features = '{params['classifier__max_features']}', \"\n",
    "                    f\"class_weight = 'balanced'\")\n",
    "    else:  # xgboost\n",
    "        param_str = (f\"learning_rate = {params['classifier__learning_rate']}, \"\n",
    "                    f\"max_depth = {params['classifier__max_depth']}, \"\n",
    "                    f\"n_estimators = {params['classifier__n_estimators']}, \"\n",
    "                    f\"subsample = {params['classifier__subsample']}, \"\n",
    "                    f\"colsample_bytree = {params['classifier__colsample_bytree']}, \"\n",
    "                    f\"scale_pos_weight = {data_info['n_samples'] * (1 - data_info['fraud_rate']) / (data_info['n_samples'] * data_info['fraud_rate']):.0f}\")\n",
    "\n",
    "    # Get CV scores\n",
    "    auc_mean = grid_search.cv_results_['mean_test_auc_roc'][best_idx]\n",
    "    auc_std = grid_search.cv_results_['std_test_auc_roc'][best_idx]\n",
    "    f1_mean = grid_search.cv_results_['mean_test_f1'][best_idx]\n",
    "    f1_std = grid_search.cv_results_['std_test_f1'][best_idx]\n",
    "\n",
    "    table_data.append({\n",
    "        'Model': model_name.replace('_', ' ').title(),\n",
    "        'Optimal Parameters': param_str,\n",
    "        'CV AUC (Mean ± SD)': f\"{auc_mean:.3f} ± {auc_std:.3f}\",\n",
    "        'CV F1-Score (Mean ± SD)': f\"{f1_mean:.3f} ± {f1_std:.3f}\"\n",
    "    })\n",
    "\n",
    "# Create and display table\n",
    "table_4_4 = pd.DataFrame(table_data)\n",
    "print(\"\\nTable 4.4: Optimal Hyperparameters and Cross-Validation Performance\")\n",
    "print(\"=\"*100)\n",
    "print(table_4_4.to_string(index=False))\n",
    "\n",
    "# Save table\n",
    "table_4_4.to_csv('../data/processed/table_4_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for best models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, model) in enumerate(best_models.items()):\n",
    "    # Get CV results\n",
    "    grid_search = joblib.load(f'../models/grid_search_{model_name}.pkl')\n",
    "    cv_result = pd.DataFrame(grid_search.cv_results_)\n",
    "    best_idx = cv_result[f'rank_test_{primary_metric}'] == 1\n",
    "\n",
    "    # Plot training vs validation scores\n",
    "    ax = axes[idx*2]\n",
    "    ax.plot(range(3), [cv_result[f'split{i}_train_auc_roc'][best_idx].values[0] for i in range(3)],\n",
    "            'b-', label='Training AUC')\n",
    "    ax.plot(range(3), [cv_result[f'split{i}_test_auc_roc'][best_idx].values[0] for i in range(3)],\n",
    "            'r-', label='Validation AUC')\n",
    "    ax.set_xlabel('CV Fold')\n",
    "    ax.set_ylabel('AUC-ROC')\n",
    "    ax.set_title(f'{model_name.replace(\"_\", \" \").title()} - AUC Convergence')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot F1 scores\n",
    "    ax = axes[idx*2 + 1]\n",
    "    ax.plot(range(3), [cv_result[f'split{i}_train_f1'][best_idx].values[0] for i in range(3)],\n",
    "            'b-', label='Training F1')\n",
    "    ax.plot(range(3), [cv_result[f'split{i}_test_f1'][best_idx].values[0] for i in range(3)],\n",
    "            'r-', label='Validation F1')\n",
    "    ax.set_xlabel('CV Fold')\n",
    "    ax.set_ylabel('F1-Score')\n",
    "    ax.set_title(f'{model_name.replace(\"_\", \" \").title()} - F1 Convergence')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/images/training_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate on Hold-out Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores = {}\n",
    "\n",
    "for model_name, model in best_models.items():\n",
    "    # Get predictions\n",
    "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_scores = {\n",
    "        'auc_roc': roc_auc_score(y_val, y_val_proba),\n",
    "        'auc_pr': average_precision_score(y_val, y_val_proba),\n",
    "        'f1': f1_score(y_val, y_val_pred),\n",
    "        'precision': precision_score(y_val, y_val_pred),\n",
    "        'recall': recall_score(y_val, y_val_pred)\n",
    "    }\n",
    "\n",
    "    validation_scores[model_name] = val_scores\n",
    "\n",
    "    print(f\"\\n{model_name.upper()} Validation Set Performance:\")\n",
    "    for metric, score in val_scores.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "# Save validation scores\n",
    "joblib.dump(validation_scores, '../results/validation_scores.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results = {\n",
    "    'best_models': best_models,\n",
    "    'cv_results': cv_results,\n",
    "    'training_times': training_times,\n",
    "    'validation_scores': validation_scores,\n",
    "    'table_4_4': table_4_4\n",
    "}\n",
    "\n",
    "joblib.dump(results, '../models/optimization_results.pkl')\n",
    "\n",
    "# Save individual best models\n",
    "for model_name, model in best_models.items():\n",
    "    joblib.dump(model, f'../models/best_model_{model_name}.pkl')\n",
    "\n",
    "print(\"All models and results saved successfully!\")\n",
    "print(f\"\\nTotal optimization time: {sum(training_times.values())/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSUMMARY OF HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name in ['logistic_regression', 'random_forest', 'xgboost']:\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(f\"Training time: {training_times[model_name]:.2f} seconds\")\n",
    "    print(f\"Best CV AUC-ROC: {validation_scores[model_name]['auc_roc']:.4f}\")\n",
    "    print(f\"Best CV F1-Score: {validation_scores[model_name]['f1']:.4f}\")\n",
    "    print(f\"Configurations tested: {len(cv_results[model_name])}\")\n",
    "\n",
    "print(\"\\nReady for test set evaluation in next notebook!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}