{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# NIBSS Fraud Detection: Model Evaluation and Performance Analysis\n",
        "\n",
        "This notebook provides comprehensive evaluation of optimized fraud detection models including:\n",
        "- Statistical significance testing with bootstrap confidence intervals\n",
        "- ROC and Precision-Recall curve analysis\n",
        "- Confusion matrix visualization\n",
        "- McNemar's test for model comparison\n",
        "- Cross-validation stability analysis\n",
        "\n",
        "The analysis includes proper statistical testing to ensure robust model performance assessment for the Nigerian banking fraud detection system."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-header",
      "metadata": {},
      "source": [
        "## Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, f1_score,\n",
        "    confusion_matrix, classification_report, precision_recall_curve,\n",
        "    roc_curve, auc, precision_score, recall_score, accuracy_score\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Load data and models\n",
        "data_splits = joblib.load('../data/processed/data_splits.pkl')\n",
        "best_models = joblib.load('../models/optimization_results.pkl')['best_models']\n",
        "\n",
        "X_test = data_splits['X_test']\n",
        "y_test = data_splits['y_test']\n",
        "\n",
        "print(\"Models and test data loaded successfully!\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"Test fraud rate: {y_test.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bootstrap-header",
      "metadata": {},
      "source": [
        "## Bootstrap Confidence Intervals\n",
        "\n",
        "We implement bootstrap resampling to calculate robust confidence intervals for our performance metrics, ensuring statistical reliability of our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bootstrap-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "def bootstrap_metric(y_true, y_pred_proba, y_pred, metric_func, n_bootstrap=1000, confidence=0.95):\n",
        "    \"\"\"Calculate bootstrap confidence intervals for a metric\"\"\"\n",
        "    scores = []\n",
        "    n_samples = len(y_true)\n",
        "\n",
        "    for _ in range(n_bootstrap):\n",
        "        # Resample with replacement\n",
        "        indices = resample(range(n_samples), n_samples=n_samples, random_state=None)\n",
        "\n",
        "        if metric_func.__name__ in ['roc_auc_score', 'average_precision_score']:\n",
        "            score = metric_func(y_true.iloc[indices], y_pred_proba[indices])\n",
        "        else:\n",
        "            score = metric_func(y_true.iloc[indices], y_pred[indices])\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    # Calculate confidence intervals\n",
        "    alpha = 1 - confidence\n",
        "    lower = np.percentile(scores, (alpha/2) * 100)\n",
        "    upper = np.percentile(scores, (1 - alpha/2) * 100)\n",
        "    mean = np.mean(scores)\n",
        "\n",
        "    return mean, lower, upper, scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "performance-header",
      "metadata": {},
      "source": [
        "## Test Set Performance Metrics with Bootstrap Confidence Intervals\n",
        "\n",
        "Calculate comprehensive performance metrics with 95% bootstrap confidence intervals for all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "performance-calculation",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize results storage\n",
        "test_metrics = {}\n",
        "\n",
        "# Generate predictions and probabilities for the test set\n",
        "predictions = {}\n",
        "probabilities = {}\n",
        "\n",
        "for model_name, model in best_models.items():\n",
        "    print(f\"Generating predictions for {model_name} on test set...\")\n",
        "    probabilities[model_name] = model.predict_proba(X_test)[:, 1]\n",
        "    predictions[model_name] = model.predict(X_test)\n",
        "    print(f\"Predictions generated for {model_name}\")\n",
        "\n",
        "# Metrics to calculate\n",
        "metrics = {\n",
        "    'Precision': precision_score,\n",
        "    'Recall': recall_score,\n",
        "    'F1-Score': f1_score,\n",
        "    'AUC': roc_auc_score,\n",
        "    'Accuracy': accuracy_score,\n",
        "    'Specificity': lambda y_true, y_pred: recall_score(1-y_true, 1-y_pred, average='binary')\n",
        "}\n",
        "\n",
        "# Calculate metrics for each model\n",
        "for model_name in best_models.keys():\n",
        "    model_metrics = {}\n",
        "\n",
        "    for metric_name, metric_func in metrics.items():\n",
        "        if metric_name == 'AUC':\n",
        "            mean, lower, upper, _ = bootstrap_metric(\n",
        "                y_test, probabilities[model_name], predictions[model_name],\n",
        "                metric_func, n_bootstrap=1000\n",
        "            )\n",
        "        else:\n",
        "            mean, lower, upper, _ = bootstrap_metric(\n",
        "                y_test, probabilities[model_name], predictions[model_name],\n",
        "                metric_func, n_bootstrap=1000\n",
        "            )\n",
        "\n",
        "        model_metrics[metric_name] = {\n",
        "            'mean': mean,\n",
        "            'lower': lower,\n",
        "            'upper': upper\n",
        "        }\n",
        "\n",
        "    test_metrics[model_name] = model_metrics\n",
        "\n",
        "print(\"\\nBootstrap confidence intervals calculated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "table45-header",
      "metadata": {},
      "source": [
        "## Table 4.5: Test Set Performance Metrics with 95% Bootstrap Confidence Intervals\n",
        "\n",
        "Comprehensive performance summary showing mean performance and confidence intervals for all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "table45",
      "metadata": {},
      "outputs": [],
      "source": [
        "table_data = []\n",
        "\n",
        "for model_name in ['logistic_regression', 'random_forest', 'xgboost']:\n",
        "    row = {'Model': model_name.replace('_', ' ').title()}\n",
        "\n",
        "    for metric in ['Precision', 'Recall', 'F1-Score', 'AUC', 'Accuracy', 'Specificity']:\n",
        "        values = test_metrics[model_name][metric]\n",
        "        row[metric] = f\"{values['mean']:.3f}\\n[{values['lower']:.3f}, {values['upper']:.3f}]\"\n",
        "\n",
        "    table_data.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "table_4_5 = pd.DataFrame(table_data)\n",
        "\n",
        "print(\"\\nTable 4.5: Test Set Performance Metrics with 95% Bootstrap Confidence Intervals\")\n",
        "print(\"=\"*100)\n",
        "for idx, row in table_4_5.iterrows():\n",
        "    print(f\"\\n{row['Model']}:\")\n",
        "    for col in ['Precision', 'Recall', 'F1-Score', 'AUC', 'Accuracy', 'Specificity']:\n",
        "        print(f\"  {col}: {row[col]}\")\n",
        "\n",
        "# Save table\n",
        "table_4_5.to_csv('../data/processed/table_4_5.csv', index=False)\n",
        "print(\"\\nTable 4.5 saved to ../data/processed/table_4_5.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thresholds-header",
      "metadata": {},
      "source": [
        "## Optimal Threshold Selection\n",
        "\n",
        "Find optimal thresholds using Youden's J statistic to maximize sensitivity + specificity - 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "threshold-optimization",
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_optimal_threshold(y_true, y_proba, metric='f1'):\n",
        "    \"\"\"Find threshold that maximizes the specified metric\"\"\"\n",
        "    thresholds = np.linspace(0, 1, 1000)\n",
        "    scores = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_proba >= threshold).astype(int)\n",
        "\n",
        "        if metric == 'f1':\n",
        "            score = f1_score(y_true, y_pred)\n",
        "        elif metric == 'youden':\n",
        "            # Youden's J = Sensitivity + Specificity - 1\n",
        "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "            score = sensitivity + specificity - 1\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    optimal_idx = np.argmax(scores)\n",
        "    return thresholds[optimal_idx], scores[optimal_idx]\n",
        "\n",
        "# Find optimal thresholds for each model\n",
        "optimal_thresholds = {}\n",
        "for model_name in best_models.keys():\n",
        "    threshold, score = find_optimal_threshold(y_test, probabilities[model_name], metric='youden')\n",
        "    optimal_thresholds[model_name] = threshold\n",
        "    print(f\"{model_name}: Optimal threshold = {threshold:.3f}\")\n",
        "\n",
        "# Save optimal thresholds\n",
        "joblib.dump(optimal_thresholds, '../models/optimal_thresholds.pkl')\n",
        "print(\"\\nOptimal thresholds saved to ../models/optimal_thresholds.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confusion-header",
      "metadata": {},
      "source": [
        "## Table 4.6: Confusion Matrices at Optimal Thresholds\n",
        "\n",
        "Detailed confusion matrices showing model performance at optimized decision thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confusion-matrices",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions at optimal thresholds\n",
        "optimal_predictions = {}\n",
        "for model_name, threshold in optimal_thresholds.items():\n",
        "    optimal_predictions[model_name] = (probabilities[model_name] >= threshold).astype(int)\n",
        "\n",
        "# Calculate confusion matrices\n",
        "confusion_matrices = {}\n",
        "for model_name in best_models.keys():\n",
        "    cm = confusion_matrix(y_test, optimal_predictions[model_name])\n",
        "    confusion_matrices[model_name] = cm\n",
        "\n",
        "# Create Table 4.6 - Confusion Matrices at Optimal Thresholds\n",
        "print(\"\\nTable 4.6: Confusion Matrices at Optimal Thresholds\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for model_name in ['logistic_regression', 'random_forest', 'xgboost']:\n",
        "    cm = confusion_matrices[model_name]\n",
        "    threshold = optimal_thresholds[model_name]\n",
        "\n",
        "    print(f\"\\n{model_name.replace('_', ' ').title()} (Threshold = {threshold:.2f})\")\n",
        "    print(\"-\"*50)\n",
        "    print(f\"{'':20} {'Predicted Negative':20} {'Predicted Positive':20}\")\n",
        "    print(f\"{'Actual Negative':20} {cm[0,0]:20,d} {cm[0,1]:20,d}\")\n",
        "    print(f\"{'Actual Positive':20} {cm[1,0]:20,d} {cm[1,1]:20,d}\")\n",
        "\n",
        "# Save confusion matrices\n",
        "table_4_6 = pd.DataFrame({\n",
        "    'Model': [name.replace('_', ' ').title() for name in confusion_matrices.keys()],\n",
        "    'Threshold': [optimal_thresholds[name] for name in confusion_matrices.keys()],\n",
        "    'TN': [cm[0,0] for cm in confusion_matrices.values()],\n",
        "    'FP': [cm[0,1] for cm in confusion_matrices.values()],\n",
        "    'FN': [cm[1,0] for cm in confusion_matrices.values()],\n",
        "    'TP': [cm[1,1] for cm in confusion_matrices.values()]\n",
        "})\n",
        "table_4_6.to_csv('../data/processed/table_4_6.csv', index=False)\n",
        "print(\"\\nTable 4.6 saved to ../data/processed/table_4_6.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "roc-header",
      "metadata": {},
      "source": [
        "## Figure 4.6: ROC Curves for Model Comparison\n",
        "\n",
        "Receiver Operating Characteristic curves showing discrimination capability across all decision thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "roc-curves",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot ROC curves for each model\n",
        "for model_name, model_label in [('logistic_regression', 'Logistic Regression'),\n",
        "                                ('random_forest', 'Random Forest'),\n",
        "                                ('xgboost', 'XGBoost')]:\n",
        "    fpr, tpr, _ = roc_curve(y_test, probabilities[model_name])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, linewidth=2,\n",
        "             label=f'{model_label} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "# Plot random classifier\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('Figure 4.6: ROC Curves for Fraud Detection Models', fontsize=14)\n",
        "plt.legend(loc=\"lower right\", fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../docs/images/figure_4_6_roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pr-header",
      "metadata": {},
      "source": [
        "## Figure 4.7: Precision-Recall Curves\n",
        "\n",
        "Precision-Recall curves showing model performance in the imbalanced dataset context, where precision-recall analysis is often more informative than ROC analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pr-curves",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot PR curves for each model\n",
        "for model_name, model_label in [('logistic_regression', 'Logistic Regression'),\n",
        "                                ('random_forest', 'Random Forest'),\n",
        "                                ('xgboost', 'XGBoost')]:\n",
        "    precision, recall, _ = precision_recall_curve(y_test, probabilities[model_name])\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.plot(recall, precision, linewidth=2,\n",
        "             label=f'{model_label} (AUC = {pr_auc:.3f})')\n",
        "\n",
        "# Plot baseline (random classifier)\n",
        "baseline = y_test.mean()\n",
        "plt.plot([0, 1], [baseline, baseline], 'k--', linewidth=1,\n",
        "         label=f'Random Classifier (AUC = {baseline:.3f})')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Figure 4.7: Precision-Recall Curves Showing Performance in Imbalanced Dataset', fontsize=14)\n",
        "plt.legend(loc=\"upper right\", fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../docs/images/figure_4_7_pr_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confusion-viz-header",
      "metadata": {},
      "source": [
        "## Figure 4.7.1: Confusion Matrices Visualization\n",
        "\n",
        "Visual representation of confusion matrices at optimal thresholds with both raw counts and normalized percentages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confusion-visualization",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "model_names_display = ['Logistic Regression', 'Random Forest', 'XGBoost']\n",
        "\n",
        "for idx, (model_name, display_name) in enumerate(zip(['logistic_regression', 'random_forest', 'xgboost'],\n",
        "                                                     model_names_display)):\n",
        "    cm = confusion_matrices[model_name]\n",
        "\n",
        "    # Normalize confusion matrix\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                xticklabels=['Legitimate', 'Fraud'],\n",
        "                yticklabels=['Legitimate', 'Fraud'],\n",
        "                cbar_kws={'label': 'Count'})\n",
        "\n",
        "    # Add normalized values as text\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            axes[idx].text(j + 0.5, i + 0.7, f'({cm_normalized[i, j]:.2%})',\n",
        "                          ha='center', va='center', fontsize=9, color='red')\n",
        "\n",
        "    axes[idx].set_title(f'{display_name}\\n(Threshold = {optimal_thresholds[model_name]:.3f})', fontsize=12)\n",
        "    axes[idx].set_xlabel('Predicted', fontsize=11)\n",
        "    axes[idx].set_ylabel('Actual', fontsize=11)\n",
        "\n",
        "plt.suptitle('Confusion Matrices Visualization', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../docs/images/figure_4_7_1_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mcnemar-header",
      "metadata": {},
      "source": [
        "## McNemar's Test for Statistical Model Comparison\n",
        "\n",
        "McNemar's test provides statistical significance testing for comparing the performance of different classifiers on the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mcnemar-test",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create function for McNemar's test\n",
        "def perform_mcnemar_test(y_pred1, y_pred2, y_true):\n",
        "    \"\"\"Perform McNemar's test between two models\"\"\"\n",
        "    # Create contingency table\n",
        "    correct1_wrong2 = np.sum((y_pred1 == y_true) & (y_pred2 != y_true))\n",
        "    wrong1_correct2 = np.sum((y_pred1 != y_true) & (y_pred2 == y_true))\n",
        "\n",
        "    # McNemar's test\n",
        "    result = mcnemar([[0, correct1_wrong2], [wrong1_correct2, 0]], exact=False)\n",
        "\n",
        "    return {\n",
        "        'b': correct1_wrong2,\n",
        "        'c': wrong1_correct2,\n",
        "        'statistic': result.statistic,\n",
        "        'p_value': result.pvalue\n",
        "    }\n",
        "\n",
        "# Perform pairwise comparisons\n",
        "comparisons = [\n",
        "    ('logistic_regression', 'random_forest', 'Logistic vs Random Forest'),\n",
        "    ('logistic_regression', 'xgboost', 'Logistic vs XGBoost'),\n",
        "    ('random_forest', 'xgboost', 'Random Forest vs XGBoost')\n",
        "]\n",
        "\n",
        "mcnemar_results = []\n",
        "\n",
        "for model1, model2, comparison_name in comparisons:\n",
        "    result = perform_mcnemar_test(\n",
        "        optimal_predictions[model1],\n",
        "        optimal_predictions[model2],\n",
        "        y_test\n",
        "    )\n",
        "\n",
        "    # Determine conclusion\n",
        "    if result['p_value'] < 0.001:\n",
        "        # Determine which model is better\n",
        "        if result['c'] > result['b']:\n",
        "            conclusion = f\"{model2.replace('_', ' ').title()} significantly better\"\n",
        "        else:\n",
        "            conclusion = f\"{model1.replace('_', ' ').title()} significantly better\"\n",
        "    else:\n",
        "        conclusion = \"No significant difference\"\n",
        "\n",
        "    mcnemar_results.append({\n",
        "        'Model Comparison': comparison_name,\n",
        "        'b (Model 1 wrong, Model 2 correct)': result['b'],\n",
        "        'c (Model 1 correct, Model 2 wrong)': result['c'],\n",
        "        'χ² Statistic': f\"{result['statistic']:.2f}\",\n",
        "        'p-value': '< 0.001' if result['p_value'] < 0.001 else f\"{result['p_value']:.4f}\",\n",
        "        'Conclusion': conclusion\n",
        "    })\n",
        "\n",
        "# Create Table 4.7\n",
        "table_4_7 = pd.DataFrame(mcnemar_results)\n",
        "\n",
        "print(\"\\nTable 4.7: McNemar's Test Results\")\n",
        "print(\"=\"*100)\n",
        "print(table_4_7.to_string(index=False))\n",
        "\n",
        "# Save table\n",
        "table_4_7.to_csv('../data/processed/table_4_7.csv', index=False)\n",
        "print(\"\\nTable 4.7 saved to ../data/processed/table_4_7.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stability-header",
      "metadata": {},
      "source": [
        "## Cross-Validation Stability Analysis\n",
        "\n",
        "Analysis of model stability across cross-validation folds to assess consistency of performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stability-analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CV results from optimization\n",
        "optimization_results = joblib.load('../models/optimization_results.pkl')\n",
        "cv_results = optimization_results['cv_results']\n",
        "primary_metric = 'roc_auc'  # Define the primary metric\n",
        "\n",
        "stability_data = []\n",
        "\n",
        "for model_name in ['logistic_regression', 'random_forest', 'xgboost']:\n",
        "    # Get best model CV results\n",
        "    model_cv = cv_results[model_name]\n",
        "    best_idx = model_cv[f'rank_test_{primary_metric}'] == 1\n",
        "\n",
        "    # Extract AUC scores for each fold\n",
        "    auc_scores = [model_cv[f'split{i}_test_auc_roc'][best_idx].values[0] for i in range(3)]\n",
        "    f1_scores = [model_cv[f'split{i}_test_f1'][best_idx].values[0] for i in range(3)]\n",
        "\n",
        "    # Calculate statistics\n",
        "    auc_mean = np.mean(auc_scores)\n",
        "    auc_std = np.std(auc_scores)\n",
        "    auc_cv = (auc_std / auc_mean) * 100  # Coefficient of variation\n",
        "\n",
        "    f1_mean = np.mean(f1_scores)\n",
        "    f1_std = np.std(f1_scores)\n",
        "    f1_cv = (f1_std / f1_mean) * 100\n",
        "\n",
        "    stability_data.append({\n",
        "        'Model': model_name.replace('_', ' ').title(),\n",
        "        'AUC Mean ± SD': f\"{auc_mean:.3f} ± {auc_std:.3f}\",\n",
        "        'AUC CV*': f\"{auc_cv:.2f}%\",\n",
        "        'F1-Score Mean ± SD': f\"{f1_mean:.3f} ± {f1_std:.3f}\",\n",
        "        'F1 CV*': f\"{f1_cv:.2f}%\",\n",
        "        'Stability Rank**': 0  # Will calculate after\n",
        "    })\n",
        "\n",
        "# Rank by average CV (lower is better)\n",
        "stability_df = pd.DataFrame(stability_data)\n",
        "stability_df['avg_cv'] = stability_df.apply(lambda x:\n",
        "    (float(x['AUC CV*'].replace('%', '')) + float(x['F1 CV*'].replace('%', ''))) / 2, axis=1)\n",
        "stability_df = stability_df.sort_values('avg_cv')\n",
        "stability_df['Stability Rank**'] = range(1, len(stability_df) + 1)\n",
        "stability_df = stability_df.drop('avg_cv', axis=1)\n",
        "\n",
        "# Create Table 4.8\n",
        "print(\"\\nTable 4.8: Cross-Validation Stability Analysis (3 CV Folds)\")\n",
        "print(\"=\"*80)\n",
        "print(stability_df.to_string(index=False))\n",
        "print(\"\\n*CV = Coefficient of Variation (lower is better)\")\n",
        "print(\"**Stability Rank: 1 = most stable, 3 = least stable\")\n",
        "\n",
        "# Save table\n",
        "stability_df.to_csv('../data/processed/table_4_8.csv', index=False)\n",
        "print(\"\\nTable 4.8 saved to ../data/processed/table_4_8.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bootstrap-viz-header",
      "metadata": {},
      "source": [
        "## Figure 4.8: AUC Scores with Bootstrap Confidence Intervals\n",
        "\n",
        "Violin plot visualization showing the distribution of bootstrap AUC scores with confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bootstrap-visualization",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get bootstrap distributions\n",
        "bootstrap_aucs = {}\n",
        "for model_name in best_models.keys():\n",
        "    _, _, _, scores = bootstrap_metric(\n",
        "        y_test, probabilities[model_name], predictions[model_name],\n",
        "        roc_auc_score, n_bootstrap=1000\n",
        "    )\n",
        "    bootstrap_aucs[model_name] = scores\n",
        "\n",
        "# Create violin plot\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "positions = [1, 2, 3]\n",
        "model_names_display = ['Logistic\\nRegression', 'Random\\nForest', 'XGBoost']\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "\n",
        "# Create violin plots\n",
        "parts = ax.violinplot(\n",
        "    [bootstrap_aucs['logistic_regression'],\n",
        "     bootstrap_aucs['random_forest'],\n",
        "     bootstrap_aucs['xgboost']],\n",
        "    positions=positions,\n",
        "    showmeans=True,\n",
        "    showextrema=True,\n",
        "    widths=0.7\n",
        ")\n",
        "\n",
        "# Customize violin plots\n",
        "for i, pc in enumerate(parts['bodies']):\n",
        "    pc.set_facecolor(colors[i])\n",
        "    pc.set_alpha(0.7)\n",
        "    pc.set_edgecolor('black')\n",
        "    pc.set_linewidth(1)\n",
        "\n",
        "# Customize other elements\n",
        "parts['cmeans'].set_color('black')\n",
        "parts['cmeans'].set_linewidth(2)\n",
        "parts['cbars'].set_color('black')\n",
        "parts['cmaxes'].set_color('black')\n",
        "parts['cmins'].set_color('black')\n",
        "\n",
        "# Add confidence interval boxes\n",
        "for i, model_name in enumerate(['logistic_regression', 'random_forest', 'xgboost']):\n",
        "    mean = test_metrics[model_name]['AUC']['mean']\n",
        "    lower = test_metrics[model_name]['AUC']['lower']\n",
        "    upper = test_metrics[model_name]['AUC']['upper']\n",
        "\n",
        "    # Draw confidence interval as a box\n",
        "    box_width = 0.15\n",
        "    rect = plt.Rectangle((positions[i] - box_width/2, lower),\n",
        "                        box_width, upper - lower,\n",
        "                        facecolor='white',\n",
        "                        edgecolor='black',\n",
        "                        linewidth=2,\n",
        "                        zorder=10)\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    # Add mean line\n",
        "    ax.plot([positions[i] - box_width/2, positions[i] + box_width/2],\n",
        "            [mean, mean], 'k-', linewidth=3, zorder=11)\n",
        "\n",
        "    # Add text annotations\n",
        "    ax.text(positions[i], ax.get_ylim()[1] * 0.98, f'{mean:.4f}',\n",
        "            ha='center', va='top', fontsize=11, fontweight='bold')\n",
        "    ax.text(positions[i], lower - 0.001, f'[{lower:.4f}, {upper:.4f}]',\n",
        "            ha='center', va='top', fontsize=9, rotation=0)\n",
        "\n",
        "# Calculate appropriate y-axis limits\n",
        "all_scores = np.concatenate(list(bootstrap_aucs.values()))\n",
        "y_min = min(all_scores.min(), min([test_metrics[m]['AUC']['lower'] for m in best_models.keys()])) - 0.005\n",
        "y_max = max(all_scores.max(), max([test_metrics[m]['AUC']['upper'] for m in best_models.keys()])) + 0.005\n",
        "\n",
        "ax.set_ylim([y_min, y_max])\n",
        "ax.set_xlim([0.5, 3.5])\n",
        "\n",
        "ax.set_xticks(positions)\n",
        "ax.set_xticklabels(model_names_display, fontsize=12)\n",
        "ax.set_ylabel('AUC-ROC Score', fontsize=13)\n",
        "ax.set_title('Figure 4.8: AUC Scores with 95% Bootstrap Confidence Intervals', fontsize=15, pad=20)\n",
        "ax.grid(True, axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add a legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor=colors[0], alpha=0.7, label='Logistic Regression'),\n",
        "    Patch(facecolor=colors[1], alpha=0.7, label='Random Forest'),\n",
        "    Patch(facecolor=colors[2], alpha=0.7, label='XGBoost'),\n",
        "    Patch(facecolor='white', edgecolor='black', linewidth=2, label='95% CI')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../docs/images/figure_4_8_auc_bootstrap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nBootstrap AUC Statistics:\")\n",
        "print(\"=\"*50)\n",
        "for model_name in ['logistic_regression', 'random_forest', 'xgboost']:\n",
        "    scores = bootstrap_aucs[model_name]\n",
        "    print(f\"\\n{model_name.replace('_', ' ').title()}:\")\n",
        "    print(f\"  Mean: {np.mean(scores):.4f}\")\n",
        "    print(f\"  Std: {np.std(scores):.4f}\")\n",
        "    print(f\"  Min: {np.min(scores):.4f}\")\n",
        "    print(f\"  Max: {np.max(scores):.4f}\")\n",
        "    print(f\"  95% CI: [{test_metrics[model_name]['AUC']['lower']:.4f}, \"\n",
        "          f\"{test_metrics[model_name]['AUC']['upper']:.4f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-header",
      "metadata": {},
      "source": [
        "## Save Results\n",
        "\n",
        "Compile and save all evaluation results for subsequent analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save-results",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile all results\n",
        "evaluation_results = {\n",
        "    'predictions': predictions,\n",
        "    'probabilities': probabilities,\n",
        "    'optimal_predictions': optimal_predictions,\n",
        "    'optimal_thresholds': optimal_thresholds,\n",
        "    'test_metrics': test_metrics,\n",
        "    'confusion_matrices': confusion_matrices,\n",
        "    'mcnemar_results': mcnemar_results,\n",
        "    'bootstrap_aucs': bootstrap_aucs,\n",
        "    'tables': {\n",
        "        'table_4_5': table_4_5,\n",
        "        'table_4_6': table_4_6,\n",
        "        'table_4_7': table_4_7,\n",
        "        'table_4_8': stability_df\n",
        "    }\n",
        "}\n",
        "\n",
        "joblib.dump(evaluation_results, '../models/evaluation_results.pkl')\n",
        "\n",
        "print(\"All evaluation results saved successfully!\")\n",
        "print(\"\\nFiles created:\")\n",
        "print(\"- ../models/evaluation_results.pkl\")\n",
        "print(\"- ../models/optimal_thresholds.pkl\")\n",
        "print(\"- ../data/processed/table_4_5.csv\")\n",
        "print(\"- ../data/processed/table_4_6.csv\")\n",
        "print(\"- ../data/processed/table_4_7.csv\")\n",
        "print(\"- ../data/processed/table_4_8.csv\")\n",
        "print(\"- ../docs/images/figure_4_6_roc_curves.png\")\n",
        "print(\"- ../docs/images/figure_4_7_pr_curves.png\")\n",
        "print(\"- ../docs/images/figure_4_7_1_confusion_matrices.png\")\n",
        "print(\"- ../docs/images/figure_4_8_auc_bootstrap.png\")\n",
        "print(\"\\nReady for feature importance analysis in next notebook!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}